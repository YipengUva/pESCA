
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Demo of the P-ESCA model on Gaussian-Gaussian-Bernoulli data sets</title><meta name="generator" content="MATLAB 8.5"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2019-02-16"><meta name="DC.source" content="demo_PESCA_GGB.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>Demo of the P-ESCA model on Gaussian-Gaussian-Bernoulli data sets</h1><!--introduction--><p>This doc is going to show how to simulate Gaussian-Gaussian-Bernoulli (G-G-B) data sets with underlying global, local common and distinct structures according to the ESCA model. After that, these data sets are used to illustrate how to construct a P-ESCA model and the corresponding model selection process. The documents of the used functions can be found by using the command 'doc function name'.</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Add current folder to the path</a></li><li><a href="#2">How to simulate proper G-G-B data sets</a></li><li><a href="#3">How to estimate the dispersion parameter <img src="demo_PESCA_GGB_eq14221827199139923399.png" alt="$\alpha$"> for each data set</a></li><li><a href="#4">How to set the parameters for the P-ESCA model</a></li><li><a href="#5">The model selection process of the P-ESCA model</a></li><li><a href="#6">How to fit the final model</a></li></ul></div><h2>Add current folder to the path<a name="1"></a></h2><pre class="codeinput">clear <span class="string">all</span>
current_fold = <span class="string">'C:\researchProjects\toolbox_binary_matrix_factorization'</span>;
addpath(genpath(current_fold));

current_fold = pwd;
addpath(genpath(current_fold));
</pre><h2>How to simulate proper G-G-B data sets<a name="2"></a></h2><p>G-G-B data sets are simulated according to the ESCA model. The number of samples is set to 200; the number of the variables in the three data sets are 1000, 500 and 100; the SNRs in simulating the global, local common and distinct structures are set to 1; the marginal probability is set to 0.1 to simulate unbalanced binary data sets; the noise levels (the square root of the dispersion parameter <img src="demo_PESCA_GGB_eq14221827199139923399.png" alt="$\alpha$">) are set to 1; the sparse ratio, which is the proportion of 0s in the simulated loading matrix, is set to 0; ntimes_noise, which indicates that the singular values of any specific structure are at least ntimes larger than the singular value of the corresponding noise term, is set to 0. After that, the signals, which are the singular values of the simulated structures, and the noise, which are the singular values of the corresponding noise terms, are shown in Fig.~1. The variation explained ratios of each component for every single data set are shown in Fig.~2.</p><pre class="codeinput"><span class="comment">%</span>
<span class="comment">% parameters used in the simulation</span>
n  = 200; <span class="comment">% samples</span>
ds = [1000, 500, 100]; <span class="comment">% number of variables of each data set</span>
margProb     = 0.1; <span class="comment">% used to simulate unbalanced binary data</span>

sparse_ratio = 0;   <span class="comment">% sparse ratio of the loading matrix</span>
ntimes_noise = 0;   <span class="comment">% require signal strength is ntimes larger than the noise</span>
noises = [1,1,1];   <span class="comment">% noise levels</span>
SNRgc  = 1;         <span class="comment">% SNR of the global common structure</span>
SNRlc  = [1, 1, 1]; <span class="comment">% SNRs of the local common structures</span>
SNRd   = [1, 1, 1]; <span class="comment">% SNRs of the distinct structures</span>

<span class="comment">% data simulation process</span>
seed = 1234; <span class="comment">% set a seed to reproduce the example, can be not exist</span>
[dataSimulation] = dataSimulation_GGB(n,ds,SNRgc,SNRlc,SNRd,<span class="keyword">...</span>
                   noises,margProb,ntimes_noise,sparse_ratio,seed);

X1 = dataSimulation.X(:,1:ds(1));
X2 = dataSimulation.X(:,(ds(1)+1):(sum(ds(1:2))));
X3 = dataSimulation.X(:,(sum(ds(1:2))+1):end);

<span class="comment">% characterize the singular values of the signal and noise</span>
subTitles ={<span class="string">'C123'</span>,<span class="string">'C12'</span>,<span class="string">'C13'</span>,<span class="string">'C23'</span>,<span class="string">'D1'</span>,<span class="string">'D2'</span>,<span class="string">'D3'</span>};

<span class="comment">% form the block sparse pattern</span>
blocks_sparse_index    = cell(7,1);
blocks_sparse_index{1} = 1:sum(ds);        <span class="comment">% C123</span>
blocks_sparse_index{2} = 1:sum(ds(1:2));   <span class="comment">% C12</span>
blocks_sparse_index{3} = [1:ds(1), (sum(ds(1:2))+1):sum(ds)]; <span class="comment">% C13</span>
blocks_sparse_index{4} = (sum(ds(1))+1):sum(ds); <span class="comment">% C23</span>
blocks_sparse_index{5} = 1:sum(ds(1)); <span class="comment">% D1</span>
blocks_sparse_index{6} = (ds(1)+1):(sum(ds(1:2))); <span class="comment">% D2</span>
blocks_sparse_index{7} = (sum(ds(1:2))+1):sum(ds); <span class="comment">% D3</span>

figure;
<span class="keyword">for</span> i = 1:7
    index_factors = (3*(i - 1)+1):3*i;
    Theta_factors = dataSimulation.U_simu(:,index_factors)*<span class="keyword">...</span>
                    diag(dataSimulation.D_simu(index_factors,1))*<span class="keyword">...</span>
                    dataSimulation.V_simu(:,index_factors)';
    index_variables = blocks_sparse_index{i};
    Theta_factors = Theta_factors(:,index_variables);
    E_factors     = dataSimulation.E_simu(:,index_variables);

    subplot(1,7,i);
    [~,signal_factors,~] = fastSVD(Theta_factors,3);
    signal_factors = diag(signal_factors);
    [~,noise_factors,~]  = fastSVD(E_factors,3);
    noise_factors = diag(noise_factors);
    plot(signal_factors, <span class="string">'-o'</span>); hold <span class="string">on</span>;
    plot(noise_factors, <span class="string">'-o'</span>); hold <span class="string">on</span>;
    title(subTitles{i});
	<span class="keyword">if</span> (i==1), ylabel(<span class="string">'singular value'</span>); xlabel(<span class="string">'components'</span>); <span class="keyword">end</span>;
<span class="keyword">end</span>
snapnow
disp([<span class="string">'Fig.~1: The singular values of the signal (blue dots),'</span><span class="keyword">...</span>
                                       <span class="string">'of the noise (red dots).'</span>]);

<span class="comment">% characterize the variation explained by each component for every data set</span>
figure
elementLabels = arrayfun(@(x){sprintf(<span class="string">'%0.1f%%'</span>,x)},<span class="keyword">...</span>
                              dataSimulation.varExpPCs_simu(1:3,:));
colLabels     = arrayfun(@(x){sprintf(<span class="string">'%0.2f%%'</span>,x)},<span class="keyword">...</span>
                              dataSimulation.varExpTotals_simu);
heatmap(dataSimulation.varExpPCs_simu(1:3,:), <span class="keyword">...</span>
        1:size(dataSimulation.varExpPCs_simu,2), colLabels,<span class="keyword">...</span>
        elementLabels, <span class="string">'ShowAllTicks'</span>, true, <span class="string">'Colormap'</span>, <span class="string">'red'</span>,<span class="keyword">...</span>
        <span class="string">'Colorbar'</span>, true);
title(<span class="string">'true variation explained for each data set'</span>);
snapnow
disp(<span class="string">'Fig.~2: Variation explained ratios for the three simulated data sets.'</span>);
</pre><img vspace="5" hspace="5" src="demo_PESCA_GGB_01.png" alt=""> <pre class="codeoutput">Fig.~1: The singular values of the signal (blue dots),of the noise (red dots).
</pre><img vspace="5" hspace="5" src="demo_PESCA_GGB_02.png" alt=""> <pre class="codeoutput">Fig.~2: Variation explained ratios for the three simulated data sets.
</pre><h2>How to estimate the dispersion parameter <img src="demo_PESCA_GGB_eq14221827199139923399.png" alt="$\alpha$"> for each data set<a name="3"></a></h2><p>For quantitative data set <img src="demo_PESCA_GGB_eq07862914750616374602.png" alt="$X_1$"> and <img src="demo_PESCA_GGB_eq03178221052144974582.png" alt="$X_2$">, the dispersion parameters are estimated using the PCA model. Details can be found in the supplementary information of the paper. The CV error plots of the the rank selections are shown in Fig.~3 and Fig.~4. The dispersion parameters of the binary data is 1. The estimated dispersion parameters for the three data sets are shown in alphas.</p><pre class="codeinput"><span class="comment">%</span>
<span class="comment">% data sets and data types</span>
dataSets  = {X1,X2,X3};
dataTypes = {<span class="string">'Gaussian'</span>,<span class="string">'Gaussian'</span>,<span class="string">'Bernoulli'</span>};

Rs = 5:15; <span class="comment">% searching range</span>
K  = 3;    <span class="comment">% K times repetations</span>
alphas = zeros(1,3); <span class="comment">% dispersion parameters</span>
ranks_estimation = zeros(3,K);

<span class="keyword">for</span> (i=1:3),
    X_i = dataSets{i};
    dataType_i = dataTypes{i};
    <span class="keyword">if</span> strcmp(dataType_i, <span class="string">'Gaussian'</span>),
        [est_mean,~,R_CV] = alpha_estimation(X_i,K,Rs);
        alphas(i) = est_mean;
        ranks_estimation(i,:) = R_CV;
        snapnow
        disp(sprintf([<span class="string">'Fig.~'</span> num2str(i+2) <span class="string">': CV error based model selection'</span><span class="keyword">...</span>
                                  <span class="string">' for the '</span> num2str(i) <span class="string">'-th data set'</span>]));
    <span class="keyword">else</span>
        alphas(i) = 1;
    <span class="keyword">end</span>
<span class="keyword">end</span>
alphas
</pre><img vspace="5" hspace="5" src="demo_PESCA_GGB_03.png" alt=""> <pre class="codeoutput">Fig.~3: CV error based model selection for the 1-th data set
</pre><img vspace="5" hspace="5" src="demo_PESCA_GGB_04.png" alt=""> <pre class="codeoutput">Fig.~4: CV error based model selection for the 2-th data set

alphas =

    0.9864    0.9962    1.0000

</pre><h2>How to set the parameters for the P-ESCA model<a name="4"></a></h2><p>The following parameters are used for the P-ESCA model selection. The meaning of these parameters can be found in the document of the function.</p><pre class="codeinput">fun = <span class="string">'GDP'</span>; gamma = 1; <span class="comment">% GDP penalty function</span>
<span class="comment">% fun = 'lp'; gamma = 0.5; % Lq penalty, q = 0.5</span>
<span class="comment">% fun = 'lp'; gamma = 1;   % Lq penalty, q = 1, group lasso penalty</span>

opts = [];
opts.gamma = gamma;  <span class="comment">% hyper-parameter for the used penalty</span>
opts.random_start = 0;
opts.tol_obj = 1e-6; <span class="comment">% stopping criteria</span>
opts.maxit   = 500;
opts.alphas  = alphas;
opts.R    = 50;    <span class="comment">% components used</span>
opts.threPath = 0; <span class="comment">% generaint thresholding path or not</span>
opts.type = <span class="string">'L2'</span>;  <span class="comment">% induce group sparsity</span>
<span class="comment">%opts.type = 'L1'; % induce group sparsity + slight elementwise sparsity</span>
<span class="comment">%opts.type = 'biLevel'; % induce group sparsity + elementwise sparsity</span>

opts.quiet = 1; <span class="comment">% don't show the progress when running the algorithm</span>
</pre><h2>The model selection process of the P-ESCA model<a name="5"></a></h2><p>At first, we need to find the proper searching ranges for tuning <img src="demo_PESCA_GGB_eq02600582657178762172.png" alt="$\lambda_g$"> for Gaussian data sets and <img src="demo_PESCA_GGB_eq09744236811052728693.png" alt="$\lambda_b$"> for Bernoulli data sets. After that, we first fix <img src="demo_PESCA_GGB_eq02600582657178762172.png" alt="$\lambda_g$"> to a small value and tune <img src="demo_PESCA_GGB_eq09744236811052728693.png" alt="$\lambda_b$">, then we will fix <img src="demo_PESCA_GGB_eq09744236811052728693.png" alt="$\lambda_b$"> and tune <img src="demo_PESCA_GGB_eq02600582657178762172.png" alt="$\lambda_g$">. Fig.~5 shows how the CV errors change with respect to the tuning parameters when the P-ESCA model with a group GDP penalty is used. The red cross marker indicates the point corresponding to the minimum CV error.</p><pre class="codeinput"><span class="comment">% find a proper searching range of lambda</span>
opts.maxit = 50;
lambda_g = 1; <span class="comment">% tuning parameter for Gaussian data sets</span>
lambda_b = 5; <span class="comment">% tuning parameter for Bernoulli data sets</span>
lambdas  = [ones(1,2)*lambda_g, lambda_b];

[~,~,~,~,out] = ESCA_group_concave(dataSets,dataTypes,lambdas,fun,opts);
<span class="comment">%out.varExpPCs(1:3,:)</span>

<span class="comment">% specify the searching range of lambda</span>
opts.maxit = 500;
n_treis    = 30;
lambda_g0  = 1;
lambdas_md_g = logspace(log10(1),log10(500),n_treis); <span class="comment">% lambda_g</span>
lambdas_md_b = logspace(log10(1),log10(100),n_treis); <span class="comment">% lambda_b</span>

<span class="comment">% model selection process</span>
[cvErrors_g,cvErrors_b,lambdas_opt,opts_init] = <span class="keyword">...</span>
                        ESCA_modelSelection_KCV_twoSteps_bg(dataSets,dataTypes,<span class="keyword">...</span>
                        lambda_g0,lambdas_md_g,lambdas_md_b,fun,opts);

<span class="comment">% how CV errors change with respect to the value of lambda_b and lambda_g</span>
figure
subplot(1,2,1);
plot(log10(lambdas_md_b), cvErrors_b(:,2:3), <span class="string">'-o'</span>);
xlabel(<span class="string">'log10(\lambda_b)'</span>);
ylabel(<span class="string">'CV error'</span>);
legend({<span class="string">'X_2'</span>,<span class="string">'X_3'</span>},<span class="string">'Location'</span>,<span class="string">'best'</span>);
title(<span class="string">'Bernoulli data sets'</span>);

subplot(1,2,2);
plot(log10(lambdas_md_g), cvErrors_g(:,1), <span class="string">'-o'</span>);
xlabel(<span class="string">'log10(\lambda_g)'</span>);
ylabel(<span class="string">'CV error'</span>);
legend({<span class="string">'X_1'</span>},<span class="string">'Location'</span>,<span class="string">'best'</span>);
title(<span class="string">'Guassian data sets'</span>);
snapnow
disp(sprintf([<span class="string">'Fig.~5: How CV errors change during the model selection '</span> <span class="keyword">...</span>
              <span class="string">'process of the P-ESCA model.'</span>]));
</pre><pre class="codeoutput">less than 3 iteration is used.
less than 3 iteration is used.
less than 3 iteration is used.
less than 3 iteration is used.
less than 3 iteration is used.
less than 3 iteration is used.
less than 3 iteration is used.
less than 3 iteration is used.
less than 3 iteration is used.
less than 3 iteration is used.
less than 3 iteration is used.
less than 3 iteration is used.
less than 3 iteration is used.
less than 3 iteration is used.
less than 3 iteration is used.
less than 3 iteration is used.
</pre><img vspace="5" hspace="5" src="demo_PESCA_GGB_05.png" alt=""> <pre class="codeoutput">Fig.~5: How CV errors change during the model selection process of the P-ESCA model.
</pre><h2>How to fit the final model<a name="6"></a></h2><p>After selecting the model with the minimum CV error, the selected model is re-fitted on the full data set with the selected value of <img src="demo_PESCA_GGB_eq07657233533591063549.png" alt="$\lambda$"> and the outputs of the selected model as the initialization. The RV coefficients in estimating the global common (C123), local common (C12, C13, C23) and distinct structures (D1, D2, D3) are shown as RVs_structures. And the corresponding rank estimations of these structures are shown as Ranks_structures. The RMSEs in estimating the simulated <img src="demo_PESCA_GGB_eq00082970382179564537.png" alt="$\mathbf{\Theta}$">, <img src="demo_PESCA_GGB_eq15316314128587878229.png" alt="$\mathbf{\Theta}_1$">, <img src="demo_PESCA_GGB_eq17570969753884455592.png" alt="$\mathbf{\Theta}_2$"> and <img src="demo_PESCA_GGB_eq15371536774656064386.png" alt="$\mathbf{\Theta}_3$"> and <img src="demo_PESCA_GGB_eq05371638286043275527.png" alt="$\mu$"> are shown as RMSEs_parameters. Fig.~6 shows the variation explained ratios computed using the estimated parameters for the three data sets.</p><pre class="codeinput"><span class="comment">%</span>
<span class="comment">% parameters used to fit the final model</span>
opts_inner = opts_init;
opts_inner.tol_obj = 1e-8;
opts_inner.maxit   = 5000;

lambdas = lambdas_opt;
[mu,A,B,S,out] = ESCA_group_concave(dataSets,dataTypes,lambdas,fun,opts_inner);

<span class="comment">%  evaluate the performance of the final model</span>
[RVs_structures,Ranks_structures,RMSEs_parameters] = <span class="keyword">...</span>
    ESCA_evaluation_metrics(mu,A,B,S,ds,dataSimulation);
RVs_structures
Ranks_structures
RMSEs_parameters

<span class="comment">% estimated variation explainted ratios</span>
<span class="comment">% index out different structures</span>
C123_index = sum(S,1)==3;
C12_index = sum(S([1,2],:),1)== 2 - C123_index;
C13_index = sum(S([1,3],:),1)== 2 - C123_index;
C23_index = sum(S([2,3],:),1)== 2 - C123_index;
D_index = sum(S(:,:),1) == 1;
D1_index = D_index &amp; sum(S(1,:),1) == 1;
D2_index = D_index &amp; sum(S(2,:),1) == 1;
D3_index = D_index &amp; sum(S(3,:),1) == 1;

varExpPCs    = out.varExpPCs(1:3,:);
varExpTotals = out.varExpTotals(1:3);
varExpPCs_plots = [varExpPCs(:,C123_index), varExpPCs(:,C12_index),<span class="keyword">...</span>
                   varExpPCs(:,C13_index),varExpPCs(:,C23_index), <span class="keyword">...</span>
                   varExpPCs(:,D1_index), varExpPCs(:,D2_index),<span class="keyword">...</span>
                   varExpPCs(:,D3_index)];

<span class="comment">% variation explained ratios</span>
figure
elementLabels = arrayfun(@(x){sprintf(<span class="string">'%0.1f%%'</span>,x)}, varExpPCs_plots);
colLabels     = arrayfun(@(x){sprintf(<span class="string">'%0.2f%%'</span>,x)}, varExpTotals);
heatmap(varExpPCs_plots,  1:size(varExpPCs,2), colLabels, elementLabels,<span class="keyword">...</span>
        <span class="string">'ShowAllTicks'</span>, true, <span class="string">'Colormap'</span>, <span class="string">'red'</span>,<span class="keyword">...</span>
        <span class="string">'Colorbar'</span>, true);
title(<span class="string">'estimated variation explained'</span>);
xlabel(<span class="string">'components'</span>);
snapnow
disp(<span class="string">'Fig.~6: The estimated variation explained ratios.'</span>);
</pre><pre class="codeoutput">
RVs_structures =

    0.9812    0.9773    0.9932    0.9957    0.9935    0.9930    0.9235


Ranks_structures =

     2     4     3     3     3     3     2


RMSEs_parameters =

    0.0244    0.0135    0.0139    0.1002    0.0080

</pre><img vspace="5" hspace="5" src="demo_PESCA_GGB_06.png" alt=""> <pre class="codeoutput">Fig.~6: The estimated variation explained ratios.
</pre><p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2015a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Demo of the P-ESCA model on Gaussian-Gaussian-Bernoulli data sets
% This doc is going to show how to simulate Gaussian-Gaussian-Bernoulli
% (G-G-B) data sets with underlying global, local common and distinct 
% structures according to the ESCA model. After that, these data sets are
% used to illustrate how to construct a P-ESCA model and the corresponding model
% selection process. The documents of the used functions 
% can be found by using the command 'doc function name'.

%% Add current folder to the path
clear all
current_fold = 'C:\researchProjects\toolbox_binary_matrix_factorization';
addpath(genpath(current_fold));

current_fold = pwd;
addpath(genpath(current_fold));

%% How to simulate proper G-G-B data sets
% G-G-B data sets are simulated according to the ESCA model. The number of
% samples is set to 200; the number of the variables in the
% three data sets are 1000, 500 and 100; the SNRs in simulating the global,
% local common and distinct structures are set to 1; the marginal probability
% is set to 0.1 to simulate unbalanced binary data sets; the noise levels (the
% square root of the dispersion parameter $\alpha$) are set to 1; the
% sparse ratio, which is the proportion of 0s in the simulated loading
% matrix, is set to 0; ntimes_noise, which indicates that the singular values 
% of any specific structure are at least ntimes larger than the singular 
% value of the corresponding noise term, is set to 0.
% After that, the signals, which are the singular values of the simulated structures,
% and the noise, which are the singular values of the corresponding noise terms, are
% shown in Fig.~1. The variation explained ratios of each component for every
% single data set are shown in Fig.~2. 

%
% parameters used in the simulation 
n  = 200; % samples
ds = [1000, 500, 100]; % number of variables of each data set
margProb     = 0.1; % used to simulate unbalanced binary data

sparse_ratio = 0;   % sparse ratio of the loading matrix
ntimes_noise = 0;   % require signal strength is ntimes larger than the noise
noises = [1,1,1];   % noise levels
SNRgc  = 1;         % SNR of the global common structure
SNRlc  = [1, 1, 1]; % SNRs of the local common structures
SNRd   = [1, 1, 1]; % SNRs of the distinct structures

% data simulation process
seed = 1234; % set a seed to reproduce the example, can be not exist
[dataSimulation] = dataSimulation_GGB(n,ds,SNRgc,SNRlc,SNRd,...
                   noises,margProb,ntimes_noise,sparse_ratio,seed);
 
X1 = dataSimulation.X(:,1:ds(1));
X2 = dataSimulation.X(:,(ds(1)+1):(sum(ds(1:2))));
X3 = dataSimulation.X(:,(sum(ds(1:2))+1):end);

% characterize the singular values of the signal and noise
subTitles ={'C123','C12','C13','C23','D1','D2','D3'};

% form the block sparse pattern
blocks_sparse_index    = cell(7,1);
blocks_sparse_index{1} = 1:sum(ds);        % C123
blocks_sparse_index{2} = 1:sum(ds(1:2));   % C12
blocks_sparse_index{3} = [1:ds(1), (sum(ds(1:2))+1):sum(ds)]; % C13
blocks_sparse_index{4} = (sum(ds(1))+1):sum(ds); % C23
blocks_sparse_index{5} = 1:sum(ds(1)); % D1
blocks_sparse_index{6} = (ds(1)+1):(sum(ds(1:2))); % D2
blocks_sparse_index{7} = (sum(ds(1:2))+1):sum(ds); % D3

figure;
for i = 1:7
    index_factors = (3*(i - 1)+1):3*i;
    Theta_factors = dataSimulation.U_simu(:,index_factors)*...
                    diag(dataSimulation.D_simu(index_factors,1))*...
                    dataSimulation.V_simu(:,index_factors)';
    index_variables = blocks_sparse_index{i};
    Theta_factors = Theta_factors(:,index_variables);
    E_factors     = dataSimulation.E_simu(:,index_variables);
    
    subplot(1,7,i);
    [~,signal_factors,~] = fastSVD(Theta_factors,3);
    signal_factors = diag(signal_factors);
    [~,noise_factors,~]  = fastSVD(E_factors,3);
    noise_factors = diag(noise_factors);
    plot(signal_factors, '-o'); hold on;
    plot(noise_factors, '-o'); hold on;
    title(subTitles{i});
	if (i==1), ylabel('singular value'); xlabel('components'); end;
end
snapnow
disp(['Fig.~1: The singular values of the signal (blue dots),'...
                                       'of the noise (red dots).']);

% characterize the variation explained by each component for every data set
figure
elementLabels = arrayfun(@(x){sprintf('%0.1f%%',x)},...
                              dataSimulation.varExpPCs_simu(1:3,:));
colLabels     = arrayfun(@(x){sprintf('%0.2f%%',x)},...
                              dataSimulation.varExpTotals_simu);
heatmap(dataSimulation.varExpPCs_simu(1:3,:), ...
        1:size(dataSimulation.varExpPCs_simu,2), colLabels,...
        elementLabels, 'ShowAllTicks', true, 'Colormap', 'red',...
        'Colorbar', true);
title('true variation explained for each data set');
snapnow
disp('Fig.~2: Variation explained ratios for the three simulated data sets.'); 

%% How to estimate the dispersion parameter $\alpha$ for each data set
% For quantitative data set $X_1$ and $X_2$, the dispersion parameters are 
% estimated using the PCA model. Details can be found in the supplementary 
% information of the paper. The CV error plots of the the rank selections 
% are shown in Fig.~3 and Fig.~4. The dispersion parameters of the binary data 
% is 1. The estimated dispersion parameters for the three data sets 
% are shown in alphas.

% 
% data sets and data types
dataSets  = {X1,X2,X3};
dataTypes = {'Gaussian','Gaussian','Bernoulli'};

Rs = 5:15; % searching range
K  = 3;    % K times repetations
alphas = zeros(1,3); % dispersion parameters
ranks_estimation = zeros(3,K);

for (i=1:3),
    X_i = dataSets{i};
    dataType_i = dataTypes{i};
    if strcmp(dataType_i, 'Gaussian'),
        [est_mean,~,R_CV] = alpha_estimation(X_i,K,Rs);
        alphas(i) = est_mean;
        ranks_estimation(i,:) = R_CV;
        snapnow
        disp(sprintf(['Fig.~' num2str(i+2) ': CV error based model selection'...
                                  ' for the ' num2str(i) '-th data set']));
    else
        alphas(i) = 1;
    end
end
alphas
 
%% How to set the parameters for the P-ESCA model
% The following parameters are used for the P-ESCA model selection. The
% meaning of these parameters can be found in the document of the function.

fun = 'GDP'; gamma = 1; % GDP penalty function
% fun = 'lp'; gamma = 0.5; % Lq penalty, q = 0.5
% fun = 'lp'; gamma = 1;   % Lq penalty, q = 1, group lasso penalty

opts = [];
opts.gamma = gamma;  % hyper-parameter for the used penalty
opts.random_start = 0;
opts.tol_obj = 1e-6; % stopping criteria 
opts.maxit   = 500;
opts.alphas  = alphas;
opts.R    = 50;    % components used 
opts.threPath = 0; % generaint thresholding path or not
opts.type = 'L2';  % induce group sparsity
%opts.type = 'L1'; % induce group sparsity + slight elementwise sparsity
%opts.type = 'biLevel'; % induce group sparsity + elementwise sparsity

opts.quiet = 1; % don't show the progress when running the algorithm

%% The model selection process of the P-ESCA model
% At first, we need to find the proper searching ranges for tuning
% $\lambda_g$ for Gaussian data sets and $\lambda_b$ for Bernoulli data
% sets. After that, we first fix $\lambda_g$ to a small value and tune
% $\lambda_b$, then we will fix $\lambda_b$ and tune $\lambda_g$. Fig.~5 
% shows how the CV errors change with respect to the tuning parameters 
% when the P-ESCA model with a group GDP penalty is used. The red cross marker 
% indicates the point corresponding to the minimum CV error. 

% find a proper searching range of lambda
opts.maxit = 50;
lambda_g = 1; % tuning parameter for Gaussian data sets
lambda_b = 5; % tuning parameter for Bernoulli data sets
lambdas  = [ones(1,2)*lambda_g, lambda_b];

[~,~,~,~,out] = ESCA_group_concave(dataSets,dataTypes,lambdas,fun,opts);
%out.varExpPCs(1:3,:)

% specify the searching range of lambda
opts.maxit = 500;
n_treis    = 30;
lambda_g0  = 1;
lambdas_md_g = logspace(log10(1),log10(500),n_treis); % lambda_g 
lambdas_md_b = logspace(log10(1),log10(100),n_treis); % lambda_b

% model selection process
[cvErrors_g,cvErrors_b,lambdas_opt,opts_init] = ...
                        ESCA_modelSelection_KCV_twoSteps_bg(dataSets,dataTypes,...
                        lambda_g0,lambdas_md_g,lambdas_md_b,fun,opts);
                    
% how CV errors change with respect to the value of lambda_b and lambda_g
figure
subplot(1,2,1);
plot(log10(lambdas_md_b), cvErrors_b(:,2:3), '-o');
xlabel('log10(\lambda_b)');
ylabel('CV error');
legend({'X_2','X_3'},'Location','best');
title('Bernoulli data sets');
    
subplot(1,2,2);
plot(log10(lambdas_md_g), cvErrors_g(:,1), '-o');
xlabel('log10(\lambda_g)');
ylabel('CV error');
legend({'X_1'},'Location','best');
title('Guassian data sets');	
snapnow
disp(sprintf(['Fig.~5: How CV errors change during the model selection ' ...
              'process of the P-ESCA model.']));

%% How to fit the final model
% After selecting the model with the minimum CV error, the selected model is
% re-fitted on the full data set with the selected value of $\lambda$ and
% the outputs of the selected model as the initialization. The RV
% coefficients in estimating the global common (C123), local common (C12,
% C13, C23) and distinct structures (D1, D2, D3) are shown as RVs_structures. 
% And the corresponding rank estimations of these structures are shown as 
% Ranks_structures. The RMSEs in estimating the simulated
% $\mathbf{\Theta}$, $\mathbf{\Theta}_1$, $\mathbf{\Theta}_2$ and
% $\mathbf{\Theta}_3$ and $\mu$ are shown as RMSEs_parameters. Fig.~6 shows
% the variation explained ratios computed using the estimated parameters for 
% the three data sets.

%
% parameters used to fit the final model
opts_inner = opts_init;
opts_inner.tol_obj = 1e-8;
opts_inner.maxit   = 5000;

lambdas = lambdas_opt;
[mu,A,B,S,out] = ESCA_group_concave(dataSets,dataTypes,lambdas,fun,opts_inner);

%  evaluate the performance of the final model
[RVs_structures,Ranks_structures,RMSEs_parameters] = ...
    ESCA_evaluation_metrics(mu,A,B,S,ds,dataSimulation);
RVs_structures
Ranks_structures
RMSEs_parameters

% estimated variation explainted ratios
% index out different structures
C123_index = sum(S,1)==3;
C12_index = sum(S([1,2],:),1)== 2 - C123_index;
C13_index = sum(S([1,3],:),1)== 2 - C123_index;
C23_index = sum(S([2,3],:),1)== 2 - C123_index;
D_index = sum(S(:,:),1) == 1;
D1_index = D_index & sum(S(1,:),1) == 1;
D2_index = D_index & sum(S(2,:),1) == 1;
D3_index = D_index & sum(S(3,:),1) == 1;

varExpPCs    = out.varExpPCs(1:3,:);
varExpTotals = out.varExpTotals(1:3);
varExpPCs_plots = [varExpPCs(:,C123_index), varExpPCs(:,C12_index),...
                   varExpPCs(:,C13_index),varExpPCs(:,C23_index), ...
                   varExpPCs(:,D1_index), varExpPCs(:,D2_index),...
                   varExpPCs(:,D3_index)];
               
% variation explained ratios
figure
elementLabels = arrayfun(@(x){sprintf('%0.1f%%',x)}, varExpPCs_plots);
colLabels     = arrayfun(@(x){sprintf('%0.2f%%',x)}, varExpTotals);
heatmap(varExpPCs_plots,  1:size(varExpPCs,2), colLabels, elementLabels,...
        'ShowAllTicks', true, 'Colormap', 'red',...
        'Colorbar', true);
title('estimated variation explained');
xlabel('components');
snapnow
disp('Fig.~6: The estimated variation explained ratios.');


##### SOURCE END #####
--></body></html>