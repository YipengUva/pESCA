
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Demo of the P-ESCA model on Bernoulli-Bernoulli-Bernoulli data sets</title><meta name="generator" content="MATLAB 8.5"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2019-02-16"><meta name="DC.source" content="demo_PESCA_BBB.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>Demo of the P-ESCA model on Bernoulli-Bernoulli-Bernoulli data sets</h1><!--introduction--><p>This doc is going to show how to simulate Bernoulli-Bernoulli-Bernoulli (B-B-B) data sets with underlying global, local common and distinct structures according to the ESCA model. After that, these data sets are used to illustrate how to construct a P-ESCA model and the corresponding model selection process. The documents of the used functions can be found by using the command 'doc function name'.</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Add current folder to the path</a></li><li><a href="#2">How to simulate proper B-B-B data sets</a></li><li><a href="#3">How to estimate the dispersion parameter <img src="demo_PESCA_BBB_eq14221827199139923399.png" alt="$\alpha$"> for each data set</a></li><li><a href="#4">How to set the parameters for the P-ESCA model</a></li><li><a href="#5">The model selection process of the P-ESCA model</a></li><li><a href="#6">How to fit the final model</a></li></ul></div><h2>Add current folder to the path<a name="1"></a></h2><pre class="codeinput">clear <span class="string">all</span>
current_fold = <span class="string">'C:\researchProjects\toolbox_binary_matrix_factorization'</span>;
addpath(genpath(current_fold));

current_fold = pwd;
addpath(genpath(current_fold));
</pre><h2>How to simulate proper B-B-B data sets<a name="2"></a></h2><p>B-B-B data sets are simulated according to the ESCA model. The number of samples is set to 200; the number of the variables in the three data sets are 1000, 500 and 100; the SNRs in simulating the global, local common and distinct structures are set to 1; the marginal probability is set to 0.1 to simulate unbalanced binary data sets; the sparse ratio, which is the proportion of 0s in the simulated loading matrix, is set to 0; ntimes_noise, which indicates that the singular values of any specific structure are at least ntimes larger than the singular value of the corresponding noise term, is set to 0. After that, the signals, which are the singular values of the simulated structures, and the noise, which are the singular values of the corresponding noise terms, are shown in Fig.~1. The variation explained ratios of each component for every single data set are shown in Fig.~2.</p><pre class="codeinput"><span class="comment">%</span>
<span class="comment">% parameters used in the simulation</span>
n  = 200; <span class="comment">% samples</span>
ds = [1000, 500, 100]; <span class="comment">% number of variables of each data set</span>
margProb     = 0.1; <span class="comment">% used to simulate unbalanced binary data</span>
sparse_ratio = 0;   <span class="comment">% sparse ratio of the loading matrix</span>
ntimes_noise = 0;   <span class="comment">% require signal strength is ntimes larger than the noise</span>

SNRgc = 1;         <span class="comment">% SNR of the global common structure</span>
SNRlc = [1, 1, 1]; <span class="comment">% SNRs of the local common structures</span>
SNRd  = [1, 1, 1]; <span class="comment">% SNRs of the distinct structures</span>

<span class="comment">% data simulation process</span>
seed = 1234; <span class="comment">% set a seed to reproduce the example, can be not exist</span>
[dataSimulation] = <span class="keyword">...</span>
     dataSimulation_BBB(n,ds,SNRgc,SNRlc,SNRd,<span class="keyword">...</span>
     margProb,ntimes_noise,sparse_ratio,seed);

X1 = dataSimulation.X(:,1:ds(1));
X2 = dataSimulation.X(:,(ds(1)+1):(sum(ds(1:2))));
X3 = dataSimulation.X(:,(sum(ds(1:2))+1):end);

<span class="comment">% characterize the singular values of the signal and noise</span>
subTitles ={<span class="string">'C123'</span>,<span class="string">'C12'</span>,<span class="string">'C13'</span>,<span class="string">'C23'</span>,<span class="string">'D1'</span>,<span class="string">'D2'</span>,<span class="string">'D3'</span>};

<span class="comment">% form the block sparse pattern</span>
blocks_sparse_index    = cell(7,1);
blocks_sparse_index{1} = 1:sum(ds);        <span class="comment">% C123</span>
blocks_sparse_index{2} = 1:sum(ds(1:2));   <span class="comment">% C12</span>
blocks_sparse_index{3} = [1:ds(1), (sum(ds(1:2))+1):sum(ds)]; <span class="comment">% C13</span>
blocks_sparse_index{4} = (sum(ds(1))+1):sum(ds); <span class="comment">% C23</span>
blocks_sparse_index{5} = 1:sum(ds(1)); <span class="comment">% D1</span>
blocks_sparse_index{6} = (ds(1)+1):(sum(ds(1:2))); <span class="comment">% D2</span>
blocks_sparse_index{7} = (sum(ds(1:2))+1):sum(ds); <span class="comment">% D3</span>

figure;
<span class="keyword">for</span> i = 1:7
    index_factors = (3*(i - 1)+1):3*i;
    Theta_factors = dataSimulation.U_simu(:,index_factors)*<span class="keyword">...</span>
                    diag(dataSimulation.D_simu(index_factors,1))*<span class="keyword">...</span>
                    dataSimulation.V_simu(:,index_factors)';
    index_variables = blocks_sparse_index{i};
    Theta_factors = Theta_factors(:,index_variables);
    E_factors     = dataSimulation.E_simu(:,index_variables);

    subplot(1,7,i);
    [~,signal_factors,~] = fastSVD(Theta_factors,3);
    signal_factors = diag(signal_factors);
    [~,noise_factors,~]  = fastSVD(E_factors,3);
    noise_factors = diag(noise_factors);
    plot(signal_factors, <span class="string">'-o'</span>); hold <span class="string">on</span>;
    plot(noise_factors, <span class="string">'-o'</span>); hold <span class="string">on</span>;
    title(subTitles{i});
	<span class="keyword">if</span> (i==1), ylabel(<span class="string">'singular value'</span>); xlabel(<span class="string">'components'</span>); <span class="keyword">end</span>;
<span class="keyword">end</span>
snapnow
disp([<span class="string">'Fig.~1: The singular values of the signal (blue dots),'</span><span class="keyword">...</span>
                                       <span class="string">'of the noise (red dots).'</span>]);

<span class="comment">% characterize the variation explained by each component for every data set</span>
figure
elementLabels = arrayfun(@(x){sprintf(<span class="string">'%0.1f%%'</span>,x)},<span class="keyword">...</span>
                              dataSimulation.varExpPCs_simu(1:3,:));
colLabels     = arrayfun(@(x){sprintf(<span class="string">'%0.2f%%'</span>,x)},<span class="keyword">...</span>
                              dataSimulation.varExpTotals_simu);
heatmap(dataSimulation.varExpPCs_simu(1:3,:), <span class="keyword">...</span>
        1:size(dataSimulation.varExpPCs_simu,2), colLabels,<span class="keyword">...</span>
        elementLabels, <span class="string">'ShowAllTicks'</span>, true, <span class="string">'Colormap'</span>, <span class="string">'red'</span>,<span class="keyword">...</span>
        <span class="string">'Colorbar'</span>, true);
title(<span class="string">'true variation explained for each data set'</span>);
snapnow
disp(<span class="string">'Fig.~2: Variation explained ratios for the three simulated data sets.'</span>);
</pre><img vspace="5" hspace="5" src="demo_PESCA_BBB_01.png" alt=""> <pre class="codeoutput">Fig.~1: The singular values of the signal (blue dots),of the noise (red dots).
</pre><img vspace="5" hspace="5" src="demo_PESCA_BBB_02.png" alt=""> <pre class="codeoutput">Fig.~2: Variation explained ratios for the three simulated data sets.
</pre><h2>How to estimate the dispersion parameter <img src="demo_PESCA_BBB_eq14221827199139923399.png" alt="$\alpha$"> for each data set<a name="3"></a></h2><p>The dispersion parameters of the binary data sets are are 1.</p><pre class="codeinput"><span class="comment">%</span>
<span class="comment">% data sets and data types</span>
dataSets  = {X1,X2,X3};
dataTypes = {<span class="string">'Bernoulli'</span>,<span class="string">'Bernoulli'</span>,<span class="string">'Bernoulli'</span>};

Rs = 5:15; <span class="comment">% searching range</span>
K  = 3;    <span class="comment">% K times repetations</span>
alphas = zeros(1,3); <span class="comment">% dispersion parameters</span>
ranks_estimation = zeros(3,K);

<span class="keyword">for</span> (i=1:3),
    X_i = dataSets{i};
    dataType_i = dataTypes{i};
    <span class="keyword">if</span> strcmp(dataType_i, <span class="string">'Gaussian'</span>),
        [est_mean,~,R_CV] = alpha_estimation(X_i,K,Rs);
        alphas(i) = est_mean;
        ranks_estimation(i,:) = R_CV;
        snapnow
        disp(sprintf([<span class="string">'Fig.~'</span> num2str(i+2) <span class="string">': CV error based model selection'</span><span class="keyword">...</span>
                                  <span class="string">' for the '</span> num2str(i) <span class="string">'-th data set'</span>]));
    <span class="keyword">else</span>
        alphas(i) = 1;
    <span class="keyword">end</span>
<span class="keyword">end</span>
alphas
</pre><pre class="codeoutput">
alphas =

     1     1     1

</pre><h2>How to set the parameters for the P-ESCA model<a name="4"></a></h2><p>The following parameters are used for the P-ESCA model selection. The meaning of these parameters can be found in the document of the function.</p><pre class="codeinput">fun = <span class="string">'GDP'</span>; gamma = 1; <span class="comment">% GDP penalty function</span>
<span class="comment">% fun = 'lp'; gamma = 0.5; % Lq penalty, q = 0.5</span>
<span class="comment">% fun = 'lp'; gamma = 1;   % Lq penalty, q = 1, group lasso penalty</span>

opts = [];
opts.gamma = gamma;  <span class="comment">% hyper-parameter for the used penalty</span>
opts.random_start = 0;
opts.tol_obj = 1e-6; <span class="comment">% stopping criteria</span>
opts.maxit   = 500;
opts.alphas  = alphas;
opts.R    = 50;    <span class="comment">% components used</span>
opts.threPath = 1; <span class="comment">% generaint thresholding path or not</span>
opts.type = <span class="string">'L2'</span>;  <span class="comment">% induce group sparsity</span>
<span class="comment">%opts.type = 'L1'; % induce group sparsity + slight elementwise sparsity</span>
<span class="comment">%opts.type = 'biLevel'; % induce group sparsity + elementwise sparsity</span>

opts.quiet = 1; <span class="comment">% don't show the progress when running the algorithm</span>
</pre><h2>The model selection process of the P-ESCA model<a name="5"></a></h2><p>At first, we need to find a proper searching range of <img src="demo_PESCA_BBB_eq07657233533591063549.png" alt="$\lambda$"> for the model selection. After that, 30 values of <img src="demo_PESCA_BBB_eq07657233533591063549.png" alt="$\lambda$"> are selected from this searching range equally in log-space. Then, the developed CV error based model selection procedure is used to find a proper model. Fig.~3 shows how regularization strength <img src="demo_PESCA_BBB_eq07657233533591063549.png" alt="$\lambda$"> effects the CV errors when the P-ESCA model with a group GDP penalty is used. The red cross marker indicates the point corresponding to the minimum CV error. Fig.~4 shows the thresholding paths during the model selection process. The red line indicates the selected cutting point.</p><pre class="codeinput"><span class="comment">% find a proper searching range of lambda</span>
opts.maxit  = 50;
lambda_test = 10;
lambdas     = lambda_test*ones(1,3);

[~,~,~,~,out] = ESCA_group_concave(dataSets,dataTypes,lambdas,fun,opts);
<span class="comment">%out.varExpPCs(1:3,:)</span>

<span class="comment">% specify the searching range of lambda</span>
opts.maxit = 500;
K = 1;
n_tries  = 30;
lambda_i = 1;
lambda_t = 100;
lambdas_md = logspace(log10(lambda_i),log10(lambda_t),n_tries);

<span class="comment">% model selection process</span>
[cvErrors_mat,inits,outs] = <span class="keyword">...</span>
                        ESCA_modelSelection_KCV(dataSets,<span class="keyword">...</span>
                        dataTypes,K,lambdas_md,fun,opts);

<span class="comment">% how CV errors, RMSEs and RVs change with respect to the value of lambda</span>
<span class="comment">% index out the point corresponding to the minimum CV error</span>
[~, index_min] = min(cvErrors_mat(:,1));
figure
plot(log10(lambdas_md), cvErrors_mat,<span class="string">'-o'</span>); hold <span class="string">on</span>;
plot(log10(lambdas_md(index_min)), cvErrors_mat(index_min,:),<span class="keyword">...</span>
                      <span class="string">'rx'</span>,<span class="string">'MarkerSize'</span>,10); hold <span class="string">on</span>;
xlabel(<span class="string">'log_{10}(\lambda)'</span>);
ylabel(<span class="string">'CV error'</span>);
title(<span class="string">'CV errors'</span>);
xlim([min(log10(lambdas_md)),max(log10(lambdas_md))]);
legend(<span class="string">'X'</span>,<span class="string">'X_1'</span>,<span class="string">'X_2'</span>,<span class="string">'X_3'</span>,<span class="string">'Location'</span>,<span class="string">'best'</span>)
snapnow
disp(sprintf([<span class="string">'Fig.~3: How CV errors change during the model selection '</span> <span class="keyword">...</span>
              <span class="string">'process of the P-ESCA model.'</span>]));

<span class="comment">% thresholding pathes</span>
<span class="comment">% index out the Sigmas to generate thresholding path</span>
Sigmas_threPath = zeros(3,opts.R,n_tries);
<span class="keyword">if</span> strcmp(opts.type, <span class="string">'L2'</span>),
    weights = sqrt(ds');
<span class="keyword">else</span>
    weights = ds';
<span class="keyword">end</span>

<span class="comment">% scaling the Sigmas according to the weights</span>
<span class="keyword">for</span> (i=1:n_tries),
    out_tmp = outs{i,1};
    Sigmas_tmp = out_tmp.Sigmas;
    Sigmas_threPath(:,:,i) = Sigmas_tmp ./ (sqrt(ds') *ones(1,50));
<span class="keyword">end</span>
maxLen = max(Sigmas_threPath(:))*1.1;

<span class="comment">% thresholding profiles plot</span>
figure
subplot(1,3,1)
lambdas_md_X1 = lambdas_md;
plot(log10(lambdas_md_X1), (squeeze(Sigmas_threPath(1,:,:)))',<span class="string">'-'</span>); hold <span class="string">on</span>;
plot([log10(lambdas_md_X1(index_min)),log10(lambdas_md_X1(index_min))],<span class="keyword">...</span>
     [0,  maxLen],<span class="string">'r'</span>);
title(<span class="string">'X1'</span>);
ylim([0, maxLen]);
xlim([min(log10(lambdas_md_X1)), max(log10(lambdas_md_X1))]);
xlabel(<span class="string">'log_{10}(\lambda)'</span>);
ylabel(<span class="string">'scaled L_2 norm'</span>);
subplot(1,3,2)
lambdas_md_X2 = lambdas_md;
plot(log10(lambdas_md_X2), (squeeze(Sigmas_threPath(2,:,:)))',<span class="string">'-'</span>); hold <span class="string">on</span>;
plot([log10(lambdas_md_X2(index_min)),log10(lambdas_md_X2(index_min))],<span class="keyword">...</span>
     [0, maxLen],<span class="string">'r'</span>);
title(<span class="string">'X2'</span>);
xlim([min(log10(lambdas_md_X2)), max(log10(lambdas_md_X2))]);
xlabel(<span class="string">'log_{10}(\lambda)'</span>);
ylim([0, maxLen]);
subplot(1,3,3)
lambdas_md_X3 = lambdas_md;
plot(log10(lambdas_md_X3), (squeeze(Sigmas_threPath(3,:,:)))',<span class="string">'-'</span>); hold <span class="string">on</span>;
plot([log10(lambdas_md_X3(index_min)),log10(lambdas_md_X3(index_min))],<span class="keyword">...</span>
     [0, maxLen],<span class="string">'r'</span>);
title(<span class="string">'X3'</span>);
ylim([0, maxLen]);
xlim([min(log10(lambdas_md_X3)), max(log10(lambdas_md_X3))]);
xlabel(<span class="string">'log_{10}(\lambda)'</span>);
snapnow
disp(<span class="string">'Fig.~4: Thresholding pathes of the model selection of the P-ESCA model.'</span>);
</pre><img vspace="5" hspace="5" src="demo_PESCA_BBB_03.png" alt=""> <pre class="codeoutput">Fig.~3: How CV errors change during the model selection process of the P-ESCA model.
</pre><img vspace="5" hspace="5" src="demo_PESCA_BBB_04.png" alt=""> <pre class="codeoutput">Fig.~4: Thresholding pathes of the model selection of the P-ESCA model.
</pre><h2>How to fit the final model<a name="6"></a></h2><p>After selecting the model with the minimum CV error, the selected model is re-fitted on the full data set with the selected value of <img src="demo_PESCA_BBB_eq07657233533591063549.png" alt="$\lambda$"> and the outputs of the selected model as the initialization. The RV coefficients in estimating the global common (C123), local common (C12, C13, C23) and distinct structures (D1, D2, D3) are shown as RVs_structures. And the corresponding rank estimations of these structures are shown as Ranks_structures. The RMSEs in estimating the simulated <img src="demo_PESCA_BBB_eq00082970382179564537.png" alt="$\mathbf{\Theta}$">, <img src="demo_PESCA_BBB_eq15316314128587878229.png" alt="$\mathbf{\Theta}_1$">, <img src="demo_PESCA_BBB_eq17570969753884455592.png" alt="$\mathbf{\Theta}_2$"> and <img src="demo_PESCA_BBB_eq15371536774656064386.png" alt="$\mathbf{\Theta}_3$"> and <img src="demo_PESCA_BBB_eq05371638286043275527.png" alt="$\mu$"> are shown as RMSEs_parameters. Fig.~5 shows the variation explained ratios computed using the estimated parameters for the three data sets.</p><pre class="codeinput"><span class="comment">%</span>
<span class="comment">% parameters used to fit the final model</span>
opts_inner = inits{index_min,1};
opts_inner.tol_obj = 1e-8;
opts_inner.maxit   = 5000;
opts_inner.threPath = 0;

lambdas = lambdas_md(index_min)*ones(1,length(dataSets));
[mu,A,B,S,out] = ESCA_group_concave(dataSets,dataTypes,lambdas,fun,opts_inner);

<span class="comment">%  evaluate the performance of the final model</span>
[RVs_structures,Ranks_structures,RMSEs_parameters] = <span class="keyword">...</span>
    ESCA_evaluation_metrics(mu,A,B,S,ds,dataSimulation);
RVs_structures
Ranks_structures
RMSEs_parameters

<span class="comment">% estimated variation explainted ratios</span>
<span class="comment">% index out different structures</span>
C123_index = sum(S,1)==3;
C12_index = sum(S([1,2],:),1)== 2 - C123_index;
C13_index = sum(S([1,3],:),1)== 2 - C123_index;
C23_index = sum(S([2,3],:),1)== 2 - C123_index;
D_index = sum(S(:,:),1) == 1;
D1_index = D_index &amp; sum(S(1,:),1) == 1;
D2_index = D_index &amp; sum(S(2,:),1) == 1;
D3_index = D_index &amp; sum(S(3,:),1) == 1;

varExpPCs    = out.varExpPCs(1:3,:);
varExpTotals = out.varExpTotals(1:3);
varExpPCs_plots = [varExpPCs(:,C123_index), varExpPCs(:,C12_index),<span class="keyword">...</span>
                   varExpPCs(:,C13_index),varExpPCs(:,C23_index), <span class="keyword">...</span>
                   varExpPCs(:,D1_index), varExpPCs(:,D2_index),<span class="keyword">...</span>
                   varExpPCs(:,D3_index)];

<span class="comment">% variation explained ratios</span>
figure
elementLabels = arrayfun(@(x){sprintf(<span class="string">'%0.1f%%'</span>,x)}, varExpPCs_plots);
colLabels     = arrayfun(@(x){sprintf(<span class="string">'%0.2f%%'</span>,x)}, varExpTotals);
heatmap(varExpPCs_plots,  1:size(varExpPCs,2), colLabels, elementLabels,<span class="keyword">...</span>
        <span class="string">'ShowAllTicks'</span>, true, <span class="string">'Colormap'</span>, <span class="string">'red'</span>,<span class="keyword">...</span>
        <span class="string">'Colorbar'</span>, true);
title(<span class="string">'estimated variation explained'</span>);
xlabel(<span class="string">'components'</span>);
snapnow
disp(<span class="string">'Fig.~5: The estimated variation explained ratios.'</span>);
</pre><pre class="codeoutput">
RVs_structures =

    0.9635    0.9758    0.9788    0.9791    0.9720    0.9833    0.9081


Ranks_structures =

     2     4     2     3     4     2     3


RMSEs_parameters =

    0.0796    0.0683    0.0721    0.2306    0.0317

</pre><img vspace="5" hspace="5" src="demo_PESCA_BBB_05.png" alt=""> <pre class="codeoutput">Fig.~5: The estimated variation explained ratios.
</pre><p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2015a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Demo of the P-ESCA model on Bernoulli-Bernoulli-Bernoulli data sets
% This doc is going to show how to simulate Bernoulli-Bernoulli-Bernoulli
% (B-B-B) data sets with underlying global, local common and distinct 
% structures according to the ESCA model. After that, these data sets are
% used to illustrate how to construct a P-ESCA model and the corresponding model
% selection process. The documents of the used functions 
% can be found by using the command 'doc function name'.

%% Add current folder to the path
clear all
current_fold = 'C:\researchProjects\toolbox_binary_matrix_factorization';
addpath(genpath(current_fold));

current_fold = pwd;
addpath(genpath(current_fold));

%% How to simulate proper B-B-B data sets
% B-B-B data sets are simulated according to the ESCA model. The number of
% samples is set to 200; the number of the variables in the
% three data sets are 1000, 500 and 100; the SNRs in simulating the global,
% local common and distinct structures are set to 1; the marginal probability
% is set to 0.1 to simulate unbalanced binary data sets; the
% sparse ratio, which is the proportion of 0s in the simulated loading
% matrix, is set to 0; ntimes_noise, which indicates that the singular values 
% of any specific structure are at least ntimes larger than the singular 
% value of the corresponding noise term, is set to 0.
% After that, the signals, which are the singular values of the simulated structures,
% and the noise, which are the singular values of the corresponding noise terms, are
% shown in Fig.~1. The variation explained ratios of each component for every
% single data set are shown in Fig.~2. 

%
% parameters used in the simulation 
n  = 200; % samples
ds = [1000, 500, 100]; % number of variables of each data set
margProb     = 0.1; % used to simulate unbalanced binary data
sparse_ratio = 0;   % sparse ratio of the loading matrix
ntimes_noise = 0;   % require signal strength is ntimes larger than the noise

SNRgc = 1;         % SNR of the global common structure
SNRlc = [1, 1, 1]; % SNRs of the local common structures
SNRd  = [1, 1, 1]; % SNRs of the distinct structures

% data simulation process
seed = 1234; % set a seed to reproduce the example, can be not exist
[dataSimulation] = ...
     dataSimulation_BBB(n,ds,SNRgc,SNRlc,SNRd,...
     margProb,ntimes_noise,sparse_ratio,seed);
 
X1 = dataSimulation.X(:,1:ds(1));
X2 = dataSimulation.X(:,(ds(1)+1):(sum(ds(1:2))));
X3 = dataSimulation.X(:,(sum(ds(1:2))+1):end);

% characterize the singular values of the signal and noise
subTitles ={'C123','C12','C13','C23','D1','D2','D3'};

% form the block sparse pattern
blocks_sparse_index    = cell(7,1);
blocks_sparse_index{1} = 1:sum(ds);        % C123
blocks_sparse_index{2} = 1:sum(ds(1:2));   % C12
blocks_sparse_index{3} = [1:ds(1), (sum(ds(1:2))+1):sum(ds)]; % C13
blocks_sparse_index{4} = (sum(ds(1))+1):sum(ds); % C23
blocks_sparse_index{5} = 1:sum(ds(1)); % D1
blocks_sparse_index{6} = (ds(1)+1):(sum(ds(1:2))); % D2
blocks_sparse_index{7} = (sum(ds(1:2))+1):sum(ds); % D3

figure;
for i = 1:7
    index_factors = (3*(i - 1)+1):3*i;
    Theta_factors = dataSimulation.U_simu(:,index_factors)*...
                    diag(dataSimulation.D_simu(index_factors,1))*...
                    dataSimulation.V_simu(:,index_factors)';
    index_variables = blocks_sparse_index{i};
    Theta_factors = Theta_factors(:,index_variables);
    E_factors     = dataSimulation.E_simu(:,index_variables);
    
    subplot(1,7,i);
    [~,signal_factors,~] = fastSVD(Theta_factors,3);
    signal_factors = diag(signal_factors);
    [~,noise_factors,~]  = fastSVD(E_factors,3);
    noise_factors = diag(noise_factors);
    plot(signal_factors, '-o'); hold on;
    plot(noise_factors, '-o'); hold on;
    title(subTitles{i});
	if (i==1), ylabel('singular value'); xlabel('components'); end;
end
snapnow
disp(['Fig.~1: The singular values of the signal (blue dots),'...
                                       'of the noise (red dots).']);

% characterize the variation explained by each component for every data set
figure
elementLabels = arrayfun(@(x){sprintf('%0.1f%%',x)},...
                              dataSimulation.varExpPCs_simu(1:3,:));
colLabels     = arrayfun(@(x){sprintf('%0.2f%%',x)},...
                              dataSimulation.varExpTotals_simu);
heatmap(dataSimulation.varExpPCs_simu(1:3,:), ...
        1:size(dataSimulation.varExpPCs_simu,2), colLabels,...
        elementLabels, 'ShowAllTicks', true, 'Colormap', 'red',...
        'Colorbar', true);
title('true variation explained for each data set');
snapnow
disp('Fig.~2: Variation explained ratios for the three simulated data sets.'); 

%% How to estimate the dispersion parameter $\alpha$ for each data set
% The dispersion parameters of the binary data sets are are 1.

% 
% data sets and data types
dataSets  = {X1,X2,X3};
dataTypes = {'Bernoulli','Bernoulli','Bernoulli'};

Rs = 5:15; % searching range
K  = 3;    % K times repetations
alphas = zeros(1,3); % dispersion parameters
ranks_estimation = zeros(3,K);

for (i=1:3),
    X_i = dataSets{i};
    dataType_i = dataTypes{i};
    if strcmp(dataType_i, 'Gaussian'),
        [est_mean,~,R_CV] = alpha_estimation(X_i,K,Rs);
        alphas(i) = est_mean;
        ranks_estimation(i,:) = R_CV;
        snapnow
        disp(sprintf(['Fig.~' num2str(i+2) ': CV error based model selection'...
                                  ' for the ' num2str(i) '-th data set']));
    else
        alphas(i) = 1;
    end
end
alphas
 
%% How to set the parameters for the P-ESCA model
% The following parameters are used for the P-ESCA model selection. The
% meaning of these parameters can be found in the document of the function.

fun = 'GDP'; gamma = 1; % GDP penalty function
% fun = 'lp'; gamma = 0.5; % Lq penalty, q = 0.5
% fun = 'lp'; gamma = 1;   % Lq penalty, q = 1, group lasso penalty

opts = [];
opts.gamma = gamma;  % hyper-parameter for the used penalty
opts.random_start = 0;
opts.tol_obj = 1e-6; % stopping criteria 
opts.maxit   = 500;
opts.alphas  = alphas;
opts.R    = 50;    % components used 
opts.threPath = 1; % generaint thresholding path or not
opts.type = 'L2';  % induce group sparsity
%opts.type = 'L1'; % induce group sparsity + slight elementwise sparsity
%opts.type = 'biLevel'; % induce group sparsity + elementwise sparsity

opts.quiet = 1; % don't show the progress when running the algorithm

%% The model selection process of the P-ESCA model
% At first, we need to find a proper searching range of $\lambda$ for the 
% model selection. After that, 30 values of $\lambda$ are selected from this
% searching range equally in log-space. Then, the developed CV error based
% model selection procedure is used to find a proper model. Fig.~3 shows
% how regularization strength $\lambda$ effects the CV errors when the
% P-ESCA model with a group GDP penalty is used. The red cross marker 
% indicates the point corresponding to the minimum CV error. 
% Fig.~4 shows the thresholding paths during the model selection process. 
% The red line indicates the selected cutting point.

% find a proper searching range of lambda
opts.maxit  = 50;
lambda_test = 10;
lambdas     = lambda_test*ones(1,3);

[~,~,~,~,out] = ESCA_group_concave(dataSets,dataTypes,lambdas,fun,opts);
%out.varExpPCs(1:3,:)

% specify the searching range of lambda
opts.maxit = 500;
K = 1;
n_tries  = 30;
lambda_i = 1;
lambda_t = 100;
lambdas_md = logspace(log10(lambda_i),log10(lambda_t),n_tries);

% model selection process
[cvErrors_mat,inits,outs] = ...
                        ESCA_modelSelection_KCV(dataSets,...
                        dataTypes,K,lambdas_md,fun,opts);
                    
% how CV errors, RMSEs and RVs change with respect to the value of lambda
% index out the point corresponding to the minimum CV error
[~, index_min] = min(cvErrors_mat(:,1));
figure
plot(log10(lambdas_md), cvErrors_mat,'-o'); hold on;
plot(log10(lambdas_md(index_min)), cvErrors_mat(index_min,:),...
                      'rx','MarkerSize',10); hold on;
xlabel('log_{10}(\lambda)');
ylabel('CV error');
title('CV errors');
xlim([min(log10(lambdas_md)),max(log10(lambdas_md))]);
legend('X','X_1','X_2','X_3','Location','best')
snapnow
disp(sprintf(['Fig.~3: How CV errors change during the model selection ' ...
              'process of the P-ESCA model.']));

% thresholding pathes
% index out the Sigmas to generate thresholding path
Sigmas_threPath = zeros(3,opts.R,n_tries);
if strcmp(opts.type, 'L2'),
    weights = sqrt(ds');
else
    weights = ds';
end

% scaling the Sigmas according to the weights
for (i=1:n_tries),
    out_tmp = outs{i,1};
    Sigmas_tmp = out_tmp.Sigmas;
    Sigmas_threPath(:,:,i) = Sigmas_tmp ./ (sqrt(ds') *ones(1,50));
end
maxLen = max(Sigmas_threPath(:))*1.1;

% thresholding profiles plot
figure
subplot(1,3,1)
lambdas_md_X1 = lambdas_md;
plot(log10(lambdas_md_X1), (squeeze(Sigmas_threPath(1,:,:)))','-'); hold on;
plot([log10(lambdas_md_X1(index_min)),log10(lambdas_md_X1(index_min))],...
     [0,  maxLen],'r');
title('X1');
ylim([0, maxLen]);
xlim([min(log10(lambdas_md_X1)), max(log10(lambdas_md_X1))]);
xlabel('log_{10}(\lambda)');
ylabel('scaled L_2 norm');
subplot(1,3,2)
lambdas_md_X2 = lambdas_md;
plot(log10(lambdas_md_X2), (squeeze(Sigmas_threPath(2,:,:)))','-'); hold on;
plot([log10(lambdas_md_X2(index_min)),log10(lambdas_md_X2(index_min))],...
     [0, maxLen],'r');
title('X2');
xlim([min(log10(lambdas_md_X2)), max(log10(lambdas_md_X2))]);
xlabel('log_{10}(\lambda)');
ylim([0, maxLen]);
subplot(1,3,3)
lambdas_md_X3 = lambdas_md;
plot(log10(lambdas_md_X3), (squeeze(Sigmas_threPath(3,:,:)))','-'); hold on;
plot([log10(lambdas_md_X3(index_min)),log10(lambdas_md_X3(index_min))],...
     [0, maxLen],'r');
title('X3');
ylim([0, maxLen]);
xlim([min(log10(lambdas_md_X3)), max(log10(lambdas_md_X3))]);
xlabel('log_{10}(\lambda)');
snapnow
disp('Fig.~4: Thresholding pathes of the model selection of the P-ESCA model.');

%% How to fit the final model
% After selecting the model with the minimum CV error, the selected model is
% re-fitted on the full data set with the selected value of $\lambda$ and
% the outputs of the selected model as the initialization. The RV
% coefficients in estimating the global common (C123), local common (C12,
% C13, C23) and distinct structures (D1, D2, D3) are shown as RVs_structures. 
% And the corresponding rank estimations of these structures are shown as 
% Ranks_structures. The RMSEs in estimating the simulated
% $\mathbf{\Theta}$, $\mathbf{\Theta}_1$, $\mathbf{\Theta}_2$ and
% $\mathbf{\Theta}_3$ and $\mu$ are shown as RMSEs_parameters. Fig.~5 shows
% the variation explained ratios computed using the estimated parameters for 
% the three data sets.

%
% parameters used to fit the final model
opts_inner = inits{index_min,1};
opts_inner.tol_obj = 1e-8;
opts_inner.maxit   = 5000;
opts_inner.threPath = 0;

lambdas = lambdas_md(index_min)*ones(1,length(dataSets));
[mu,A,B,S,out] = ESCA_group_concave(dataSets,dataTypes,lambdas,fun,opts_inner);

%  evaluate the performance of the final model
[RVs_structures,Ranks_structures,RMSEs_parameters] = ...
    ESCA_evaluation_metrics(mu,A,B,S,ds,dataSimulation);
RVs_structures
Ranks_structures
RMSEs_parameters

% estimated variation explainted ratios
% index out different structures
C123_index = sum(S,1)==3;
C12_index = sum(S([1,2],:),1)== 2 - C123_index;
C13_index = sum(S([1,3],:),1)== 2 - C123_index;
C23_index = sum(S([2,3],:),1)== 2 - C123_index;
D_index = sum(S(:,:),1) == 1;
D1_index = D_index & sum(S(1,:),1) == 1;
D2_index = D_index & sum(S(2,:),1) == 1;
D3_index = D_index & sum(S(3,:),1) == 1;

varExpPCs    = out.varExpPCs(1:3,:);
varExpTotals = out.varExpTotals(1:3);
varExpPCs_plots = [varExpPCs(:,C123_index), varExpPCs(:,C12_index),...
                   varExpPCs(:,C13_index),varExpPCs(:,C23_index), ...
                   varExpPCs(:,D1_index), varExpPCs(:,D2_index),...
                   varExpPCs(:,D3_index)];
               
% variation explained ratios
figure
elementLabels = arrayfun(@(x){sprintf('%0.1f%%',x)}, varExpPCs_plots);
colLabels     = arrayfun(@(x){sprintf('%0.2f%%',x)}, varExpTotals);
heatmap(varExpPCs_plots,  1:size(varExpPCs,2), colLabels, elementLabels,...
        'ShowAllTicks', true, 'Colormap', 'red',...
        'Colorbar', true);
title('estimated variation explained');
xlabel('components');
snapnow
disp('Fig.~5: The estimated variation explained ratios.');


##### SOURCE END #####
--></body></html>